{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Code_Similarity'...\r\n",
      "remote: Enumerating objects: 1055, done.\u001B[K\r\n",
      "remote: Counting objects: 100% (1052/1052), done.\u001B[K\r\n",
      "remote: Compressing objects: 100% (244/244), done.\u001B[K\r\n",
      "remote: Total 1055 (delta 394), reused 1007 (delta 352), pack-reused 3\u001B[K\r\n",
      "Receiving objects: 100% (1055/1055), 1.20 MiB | 4.35 MiB/s, done.\r\n",
      "Resolving deltas: 100% (394/394), done.\r\n",
      "/home/user/PycharmProjects/code-similarity/code-similarity/Code_Similarity/Code_Similarity/Code_Similarity/Code_Similarity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/PycharmProjects/code-similarity/code-similarity/.venv/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Dataset Triploss'\t      Triplet_loss_code.ipynb   TripletLoss.py\r\n",
      " file_similarity_scores.csv   tripletloss.py\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/FSMVU-Tubitak-1001-Kod-Analiz/Code_Similarity.git\n",
    "%cd Code_Similarity\n",
    "!ls\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-08T20:40:11.059518178Z",
     "start_time": "2024-01-08T20:40:07.222318841Z"
    }
   },
   "id": "initial_id",
   "execution_count": 92
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "# Define the directory where the 'Dataset' folder is located\n",
    "base_path = r'/home/user/PycharmProjects/code-similarity/code-similarity/Code_Similarity/Dataset Triploss'\n",
    "\n",
    "# Arrays to hold the file paths\n",
    "positive_array = []  # To store paths of 'plagiarized' files\n",
    "anchor_array = []    # To store paths of 'original' files\n",
    "negative_array = []  # To store paths of 'non-plagiarized' files\n",
    "\n",
    "# Walk through the directory structure\n",
    "for root, dirs, files in os.walk(base_path):\n",
    "    for file in files:\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(root, file)\n",
    "        # Categorize the files based on the containing directory\n",
    "        if 'plagiarized' in root:\n",
    "            positive_array.append(file_path)\n",
    "        if 'original' in root:\n",
    "            anchor_array.append(file_path)\n",
    "        if 'negative' in root:\n",
    "            negative_array.append(file_path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T06:32:02.640514916Z",
     "start_time": "2024-01-09T06:32:02.622161967Z"
    }
   },
   "id": "11c2c5ea8704b158",
   "execution_count": 112
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "('/home/user/PycharmProjects/code-similarity/code-similarity/Code_Similarity/Dataset Triploss/case-03/plagiarized/L1/01/L1.java',\n '/home/user/PycharmProjects/code-similarity/code-similarity/Code_Similarity/Dataset Triploss/case-03/original/T3.java',\n '/home/user/PycharmProjects/code-similarity/code-similarity/Code_Similarity/Dataset Triploss/case-03/negative/14/Soal3.java')"
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_array[0],anchor_array[0],negative_array[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T06:32:03.344246966Z",
     "start_time": "2024-01-09T06:32:03.312985822Z"
    }
   },
   "id": "a7e3d612f9346d3e",
   "execution_count": 113
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T06:32:04.231178449Z",
     "start_time": "2024-01-09T06:32:04.224183039Z"
    }
   },
   "id": "f0128ff652064e39",
   "execution_count": 113
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 başlıyor\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_24/roberta/pooler/dense/kernel:0', 'tf_roberta_model_24/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_24/roberta/pooler/dense/kernel:0', 'tf_roberta_model_24/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_24/roberta/pooler/dense/kernel:0', 'tf_roberta_model_24/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_24/roberta/pooler/dense/kernel:0', 'tf_roberta_model_24/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Kaybı: 1.1354544162750244\n",
      "Epoch 2 başlıyor\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_24/roberta/pooler/dense/kernel:0', 'tf_roberta_model_24/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_24/roberta/pooler/dense/kernel:0', 'tf_roberta_model_24/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_24/roberta/pooler/dense/kernel:0', 'tf_roberta_model_24/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_24/roberta/pooler/dense/kernel:0', 'tf_roberta_model_24/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Kaybı: 0.0901002436876297\n",
      "Epoch 3 başlıyor\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_24/roberta/pooler/dense/kernel:0', 'tf_roberta_model_24/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_24/roberta/pooler/dense/kernel:0', 'tf_roberta_model_24/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_24/roberta/pooler/dense/kernel:0', 'tf_roberta_model_24/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_24/roberta/pooler/dense/kernel:0', 'tf_roberta_model_24/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Kaybı: 0.05932176858186722\n",
      "Epoch 4 başlıyor\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_24/roberta/pooler/dense/kernel:0', 'tf_roberta_model_24/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_24/roberta/pooler/dense/kernel:0', 'tf_roberta_model_24/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_24/roberta/pooler/dense/kernel:0', 'tf_roberta_model_24/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_24/roberta/pooler/dense/kernel:0', 'tf_roberta_model_24/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Kaybı: 0.0\n",
      "Epoch 5 başlıyor\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_24/roberta/pooler/dense/kernel:0', 'tf_roberta_model_24/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_24/roberta/pooler/dense/kernel:0', 'tf_roberta_model_24/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_24/roberta/pooler/dense/kernel:0', 'tf_roberta_model_24/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_24/roberta/pooler/dense/kernel:0', 'tf_roberta_model_24/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Kaybı: 0.19573140144348145\n",
      "Epoch 6 başlıyor\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_24/roberta/pooler/dense/kernel:0', 'tf_roberta_model_24/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_24/roberta/pooler/dense/kernel:0', 'tf_roberta_model_24/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_24/roberta/pooler/dense/kernel:0', 'tf_roberta_model_24/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_24/roberta/pooler/dense/kernel:0', 'tf_roberta_model_24/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Kaybı: 0.0\n",
      "Epoch 7 başlıyor\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_24/roberta/pooler/dense/kernel:0', 'tf_roberta_model_24/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_24/roberta/pooler/dense/kernel:0', 'tf_roberta_model_24/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_24/roberta/pooler/dense/kernel:0', 'tf_roberta_model_24/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_24/roberta/pooler/dense/kernel:0', 'tf_roberta_model_24/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Kaybı: 0.0\n",
      "Epoch 8 başlıyor\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_24/roberta/pooler/dense/kernel:0', 'tf_roberta_model_24/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_24/roberta/pooler/dense/kernel:0', 'tf_roberta_model_24/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_24/roberta/pooler/dense/kernel:0', 'tf_roberta_model_24/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_24/roberta/pooler/dense/kernel:0', 'tf_roberta_model_24/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Kaybı: 0.0\n",
      "Epoch 9 başlıyor\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_24/roberta/pooler/dense/kernel:0', 'tf_roberta_model_24/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_24/roberta/pooler/dense/kernel:0', 'tf_roberta_model_24/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_24/roberta/pooler/dense/kernel:0', 'tf_roberta_model_24/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_24/roberta/pooler/dense/kernel:0', 'tf_roberta_model_24/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Kaybı: 0.0\n",
      "Epoch 10 başlıyor\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_24/roberta/pooler/dense/kernel:0', 'tf_roberta_model_24/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_24/roberta/pooler/dense/kernel:0', 'tf_roberta_model_24/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_24/roberta/pooler/dense/kernel:0', 'tf_roberta_model_24/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_24/roberta/pooler/dense/kernel:0', 'tf_roberta_model_24/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Kaybı: 0.0\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: code_similarity_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: code_similarity_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/user/PycharmProjects/code-similarity/code-similarity/Code_Similarity/Model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/user/PycharmProjects/code-similarity/code-similarity/Code_Similarity/Model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model /home/user/PycharmProjects/code-similarity/code-similarity/Code_Similarity/Model dizinine kaydedildi.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Gerekli Kütüphaneler\n",
    "import tensorflow as tf\n",
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "import os\n",
    "\n",
    "# CodeBERT Tokenizer ve Modelini Yükle\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "codebert_model = TFAutoModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "for layer in codebert_model.layers:\n",
    "    if 'pooler' in layer.name:\n",
    "        layer.trainable = False\n",
    "\n",
    "# Kodu tokenize etme fonksiyonu\n",
    "def tokenize_code(file_path, tokenizer):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        code = file.read()\n",
    "    inputs = tokenizer(code, return_tensors=\"tf\", truncation=True, padding='max_length', max_length=512)\n",
    "    return inputs['input_ids'], inputs['attention_mask']\n",
    "\n",
    "# Dosyaları işleyip tokenize etme fonksiyonu\n",
    "def load_and_tokenize_data(file_paths, tokenizer):\n",
    "    input_ids_list = []\n",
    "    attention_masks_list = []\n",
    "    for path in file_paths:\n",
    "        input_ids, attention_mask = tokenize_code(path, tokenizer)\n",
    "        input_ids_list.append(tf.squeeze(input_ids))\n",
    "        attention_masks_list.append(tf.squeeze(attention_mask))\n",
    "    return input_ids_list, attention_masks_list\n",
    "\n",
    "# Gömme Modelini Tanımla\n",
    "def codebert_embedding_model():\n",
    "    input_ids = tf.keras.layers.Input(shape=(512,), dtype='int32')\n",
    "    attention_mask = tf.keras.layers.Input(shape=(512,), dtype='int32')\n",
    "    embeddings = codebert_model(input_ids, attention_mask=attention_mask)[0][:,0,:]\n",
    "    return tf.keras.Model(inputs=[input_ids, attention_mask], outputs=embeddings)\n",
    "\n",
    "embedding_model = codebert_embedding_model()\n",
    "\n",
    "# Triplet Loss Fonksiyonu\n",
    "\n",
    "'''\n",
    "Bu fonksiyon, modelin anchor ile positive arasındaki mesafeyi azaltıp,\n",
    " anchor ile negative arasındaki mesafeyi artırmasını sağlayacak şekilde kaybı (loss) hesaplar.\n",
    "'''\n",
    "def triplet_loss(y_true, y_pred, alpha=0.2):\n",
    "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
    "    pos_dist = tf.reduce_sum(tf.square(anchor - positive), axis=-1)  # Anchor ile Positive arasındaki mesafe\n",
    "    neg_dist = tf.reduce_sum(tf.square(anchor - negative), axis=-1)  # Anchor ile Negative arasındaki mesafe\n",
    "    basic_loss = pos_dist - neg_dist + alpha  # Loss hesaplama\n",
    "    loss = tf.reduce_mean(tf.maximum(basic_loss, 0.0))  # Loss'un sıfırdan büyük olmasını sağlama\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Veri setlerini yükle ve tokenize et\n",
    "anchor_input_ids, anchor_attention_masks = load_and_tokenize_data(anchor_array, tokenizer)\n",
    "positive_input_ids, positive_attention_masks = load_and_tokenize_data(positive_array, tokenizer)\n",
    "negative_input_ids, negative_attention_masks = load_and_tokenize_data(negative_array, tokenizer)\n",
    "\n",
    "# Veri setini oluştur\n",
    "def create_dataset(input_ids, attention_masks):\n",
    "    return tf.data.Dataset.from_tensor_slices((input_ids, attention_masks))\n",
    "\n",
    "anchor_dataset = create_dataset(anchor_input_ids, anchor_attention_masks)\n",
    "positive_dataset = create_dataset(positive_input_ids, positive_attention_masks)\n",
    "negative_dataset = create_dataset(negative_input_ids, negative_attention_masks)\n",
    "\n",
    "triplet_dataset = tf.data.Dataset.zip((anchor_dataset, positive_dataset, negative_dataset))\n",
    "batch_size = 32  # Batch boyutunu ayarla\n",
    "triplet_dataset = triplet_dataset.batch(batch_size)\n",
    "\n",
    "# Eğitim döngüsü\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "# Toplam eğitim epoch sayısını ayarla\n",
    "num_epochs = 10\n",
    "\n",
    "'''\n",
    "Yukarıda verilen kod, bir Triplet Loss modelinin eğitimini ve modelin bir dosya olarak saklanmasını sağlar.\n",
    "Eğitim sürecinde, model \"code_similarity_model\" adı altında yerel diskte kaydedilir.\n",
    "Bu, modelin eğitim sonrası durumunu içerir ve bu modeli daha sonra kullanmak için bu dosyayı yükleyebilirsiniz.\n",
    "\n",
    "'''\n",
    "# Eğitim epoch'ları için döngü\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1} başlıyor\")  # Mevcut epoch'un başladığını bildir\n",
    "    epoch_loss = 0  # Bu epoch için toplam kaybı sıfırla\n",
    "\n",
    "    # Dataset üzerinden döngü yaparak her bir triplet için eğitim gerçekleştir\n",
    "    for step, ((anchor_input, anchor_mask), (positive_input, positive_mask), (negative_input, negative_mask)) in enumerate(triplet_dataset):\n",
    "\n",
    "        '''\n",
    "        tf.GradientTape kullanımının nedeni, TensorFlow'da otomatik türev hesaplama işlevini sağlamasıdır.\n",
    "        Derin öğrenme modelleri genellikle geri yayılım (backpropagation) yoluyla eğitilir,\n",
    "        bu süreçte modelin ağırlıkları, kayıp fonksiyonunun (loss function) gradyanlarına göre güncellenir.\n",
    "         tf.GradientTape bu gradyan hesaplamalarını kolaylaştırır.\n",
    "        '''\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Embedding modelini kullanarak anchor, positive ve negative için embedding'leri hesapla\n",
    "            anchor_embeddings = embedding_model([anchor_input, anchor_mask])\n",
    "            positive_embeddings = embedding_model([positive_input, positive_mask])\n",
    "            negative_embeddings = embedding_model([negative_input, negative_mask])\n",
    "\n",
    "            # Triplet kaybını hesapla\n",
    "            loss = triplet_loss(None, [anchor_embeddings, positive_embeddings, negative_embeddings])\n",
    "\n",
    "        # Hesaplanan kayıp üzerinden gradyanları hesapla\n",
    "        gradients = tape.gradient(loss, embedding_model.trainable_variables)\n",
    "        # Optimizer kullanarak gradyanları uygula ve model ağırlıklarını güncelle\n",
    "        optimizer.apply_gradients(zip(gradients, embedding_model.trainable_variables))\n",
    "        # Epoch kaybına bu adımdaki kaybı ekle\n",
    "        epoch_loss += loss.numpy()\n",
    "\n",
    "    # Epoch sonunda toplam kaybı yazdır\n",
    "    print(f\"Epoch {epoch+1} Kaybı: {epoch_loss}\")\n",
    "\n",
    "\n",
    "# Modeli kaydet\n",
    "embedding_model.save(\"code_similarity_model\")\n",
    "\n",
    "\n",
    "# Modeli kaydetmek için dizin yolu oluşturun\n",
    "saved_model_path = r\"/home/user/PycharmProjects/code-similarity/code-similarity/Code_Similarity/Model\"  # Burada istediğiniz bir yolu belirleyin\n",
    "if not os.path.exists(saved_model_path):\n",
    "    os.makedirs(saved_model_path)\n",
    "\n",
    "# Modeli SavedModel formatında kaydedin\n",
    "tf.saved_model.save(embedding_model, saved_model_path)\n",
    "\n",
    "print(f\"Model {saved_model_path} dizinine kaydedildi.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T06:36:56.771146962Z",
     "start_time": "2024-01-09T06:32:04.991585915Z"
    }
   },
   "id": "ac0d7eaa06c5d957",
   "execution_count": 114
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "embedding_model.save_weights('mcode_similarity_model_weights.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T06:36:58.233466620Z",
     "start_time": "2024-01-09T06:36:56.831407878Z"
    }
   },
   "id": "273f042bce4535dc",
   "execution_count": 115
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "embedding_model.load_weights('mcode_similarity_model_weights.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T06:36:58.707581795Z",
     "start_time": "2024-01-09T06:36:58.237693298Z"
    }
   },
   "id": "63f40c0bcfaffcbc",
   "execution_count": 116
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 başlıyor\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_25/roberta/pooler/dense/kernel:0', 'tf_roberta_model_25/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_25/roberta/pooler/dense/kernel:0', 'tf_roberta_model_25/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_25/roberta/pooler/dense/kernel:0', 'tf_roberta_model_25/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_25/roberta/pooler/dense/kernel:0', 'tf_roberta_model_25/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Kaybı: 0.0\n",
      "Epoch 2 başlıyor\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_25/roberta/pooler/dense/kernel:0', 'tf_roberta_model_25/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_25/roberta/pooler/dense/kernel:0', 'tf_roberta_model_25/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_25/roberta/pooler/dense/kernel:0', 'tf_roberta_model_25/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_25/roberta/pooler/dense/kernel:0', 'tf_roberta_model_25/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Kaybı: 0.0\n",
      "Epoch 3 başlıyor\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_25/roberta/pooler/dense/kernel:0', 'tf_roberta_model_25/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_25/roberta/pooler/dense/kernel:0', 'tf_roberta_model_25/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_25/roberta/pooler/dense/kernel:0', 'tf_roberta_model_25/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_25/roberta/pooler/dense/kernel:0', 'tf_roberta_model_25/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Kaybı: 0.0\n",
      "Epoch 4 başlıyor\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_25/roberta/pooler/dense/kernel:0', 'tf_roberta_model_25/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_25/roberta/pooler/dense/kernel:0', 'tf_roberta_model_25/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_25/roberta/pooler/dense/kernel:0', 'tf_roberta_model_25/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_25/roberta/pooler/dense/kernel:0', 'tf_roberta_model_25/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Kaybı: 0.0\n",
      "Epoch 5 başlıyor\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_25/roberta/pooler/dense/kernel:0', 'tf_roberta_model_25/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_25/roberta/pooler/dense/kernel:0', 'tf_roberta_model_25/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_25/roberta/pooler/dense/kernel:0', 'tf_roberta_model_25/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_25/roberta/pooler/dense/kernel:0', 'tf_roberta_model_25/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Kaybı: 0.0\n",
      "Epoch 6 başlıyor\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_25/roberta/pooler/dense/kernel:0', 'tf_roberta_model_25/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_25/roberta/pooler/dense/kernel:0', 'tf_roberta_model_25/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_25/roberta/pooler/dense/kernel:0', 'tf_roberta_model_25/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_25/roberta/pooler/dense/kernel:0', 'tf_roberta_model_25/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Kaybı: 0.0\n",
      "Epoch 7 başlıyor\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_25/roberta/pooler/dense/kernel:0', 'tf_roberta_model_25/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_25/roberta/pooler/dense/kernel:0', 'tf_roberta_model_25/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_25/roberta/pooler/dense/kernel:0', 'tf_roberta_model_25/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_25/roberta/pooler/dense/kernel:0', 'tf_roberta_model_25/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Kaybı: 0.0\n",
      "Epoch 8 başlıyor\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_25/roberta/pooler/dense/kernel:0', 'tf_roberta_model_25/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_25/roberta/pooler/dense/kernel:0', 'tf_roberta_model_25/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_25/roberta/pooler/dense/kernel:0', 'tf_roberta_model_25/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_25/roberta/pooler/dense/kernel:0', 'tf_roberta_model_25/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Kaybı: 0.0\n",
      "Epoch 9 başlıyor\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_25/roberta/pooler/dense/kernel:0', 'tf_roberta_model_25/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_25/roberta/pooler/dense/kernel:0', 'tf_roberta_model_25/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_25/roberta/pooler/dense/kernel:0', 'tf_roberta_model_25/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_25/roberta/pooler/dense/kernel:0', 'tf_roberta_model_25/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Kaybı: 0.0\n",
      "Epoch 10 başlıyor\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_25/roberta/pooler/dense/kernel:0', 'tf_roberta_model_25/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_25/roberta/pooler/dense/kernel:0', 'tf_roberta_model_25/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_25/roberta/pooler/dense/kernel:0', 'tf_roberta_model_25/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_25/roberta/pooler/dense/kernel:0', 'tf_roberta_model_25/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Kaybı: 0.0\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: new_code_similarity_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: new_code_similarity_model/assets\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "\n",
    "# Load CodeBERT Model\n",
    "codebert_model = TFAutoModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "# Rebuild the embedding model architecture (matching the saved model)\n",
    "def codebert_embedding_model():\n",
    "    input_ids = tf.keras.layers.Input(shape=(512,), dtype='int32')\n",
    "    attention_mask = tf.keras.layers.Input(shape=(512,), dtype='int32')\n",
    "    embeddings = codebert_model(input_ids, attention_mask=attention_mask)[0][:,0,:]\n",
    "    return tf.keras.Model(inputs=[input_ids, attention_mask], outputs=embeddings)\n",
    "\n",
    "# Instantiate the model\n",
    "embedding_model = codebert_embedding_model()\n",
    "\n",
    "# Load the weights from the saved model\n",
    "\n",
    "embedding_model.load_weights('mcode_similarity_model_weights.h5')\n",
    "# Triplet Loss Fonksiyonu\n",
    "\n",
    "'''\n",
    "Bu fonksiyon, modelin anchor ile positive arasındaki mesafeyi azaltıp,\n",
    " anchor ile negative arasındaki mesafeyi artırmasını sağlayacak şekilde kaybı (loss) hesaplar.\n",
    "'''\n",
    "def triplet_loss(y_true, y_pred, alpha=0.2):\n",
    "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
    "    pos_dist = tf.reduce_sum(tf.square(anchor - positive), axis=-1)  # Anchor ile Positive arasındaki mesafe\n",
    "    neg_dist = tf.reduce_sum(tf.square(anchor - negative), axis=-1)  # Anchor ile Negative arasındaki mesafe\n",
    "    basic_loss = pos_dist - neg_dist + alpha  # Loss hesaplama\n",
    "    loss = tf.reduce_mean(tf.maximum(basic_loss, 0.0))  # Loss'un sıfırdan büyük olmasını sağlama\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Veri setlerini yükle ve tokenize et\n",
    "anchor_input_ids, anchor_attention_masks = load_and_tokenize_data(anchor_array, tokenizer)\n",
    "positive_input_ids, positive_attention_masks = load_and_tokenize_data(positive_array, tokenizer)\n",
    "negative_input_ids, negative_attention_masks = load_and_tokenize_data(negative_array, tokenizer)\n",
    "\n",
    "# Veri setini oluştur\n",
    "def create_dataset(input_ids, attention_masks):\n",
    "    return tf.data.Dataset.from_tensor_slices((input_ids, attention_masks))\n",
    "\n",
    "anchor_dataset = create_dataset(anchor_input_ids, anchor_attention_masks)\n",
    "positive_dataset = create_dataset(positive_input_ids, positive_attention_masks)\n",
    "negative_dataset = create_dataset(negative_input_ids, negative_attention_masks)\n",
    "\n",
    "triplet_dataset = tf.data.Dataset.zip((anchor_dataset, positive_dataset, negative_dataset))\n",
    "batch_size = 32  # Batch boyutunu ayarla\n",
    "triplet_dataset = triplet_dataset.batch(batch_size)\n",
    "\n",
    "# Eğitim döngüsü\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "# Toplam eğitim epoch sayısını ayarla\n",
    "num_epochs = 10\n",
    "\n",
    "# Eğitim epoch'ları için döngü\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1} başlıyor\")  # Mevcut epoch'un başladığını bildir\n",
    "    epoch_loss = 0  # Bu epoch için toplam kaybı sıfırla\n",
    "\n",
    "    # Dataset üzerinden döngü yaparak her bir triplet için eğitim gerçekleştir\n",
    "    for step, ((anchor_input, anchor_mask), (positive_input, positive_mask), (negative_input, negative_mask)) in enumerate(triplet_dataset):\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Embedding modelini kullanarak anchor, positive ve negative için embedding'leri hesapla\n",
    "            anchor_embeddings = embedding_model([anchor_input, anchor_mask])\n",
    "            positive_embeddings = embedding_model([positive_input, positive_mask])\n",
    "            negative_embeddings = embedding_model([negative_input, negative_mask])\n",
    "\n",
    "            # Triplet kaybını hesapla\n",
    "            loss = triplet_loss(None, [anchor_embeddings, positive_embeddings, negative_embeddings])\n",
    "\n",
    "        # Hesaplanan kayıp üzerinden gradyanları hesapla\n",
    "        gradients = tape.gradient(loss, embedding_model.trainable_variables)\n",
    "        # Optimizer kullanarak gradyanları uygula ve model ağırlıklarını güncelle\n",
    "        optimizer.apply_gradients(zip(gradients, embedding_model.trainable_variables))\n",
    "        # Epoch kaybına bu adımdaki kaybı ekle\n",
    "        epoch_loss += loss.numpy()\n",
    "\n",
    "    # Epoch sonunda toplam kaybı yazdır\n",
    "    print(f\"Epoch {epoch+1} Kaybı: {epoch_loss}\")\n",
    "\n",
    "\n",
    "# Modeli kaydet\n",
    "embedding_model.save(\"new_code_similarity_model\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T06:41:30.300931469Z",
     "start_time": "2024-01-09T06:37:01.735151254Z"
    }
   },
   "id": "65f5ad0d89e3669c",
   "execution_count": 117
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "all_arrays = np.concatenate((positive_array, anchor_array, negative_array), axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T06:41:30.315127996Z",
     "start_time": "2024-01-09T06:41:30.303203672Z"
    }
   },
   "id": "d0fb8d9cf5d4d546",
   "execution_count": 118
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "467"
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_arrays)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T06:41:30.318584431Z",
     "start_time": "2024-01-09T06:41:30.308760316Z"
    }
   },
   "id": "6ebe763ead294997",
   "execution_count": 119
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'/home/user/PycharmProjects/code-similarity/code-similarity/Code_Similarity/Dataset Triploss/case-03/plagiarized/L1/01/L1.java'"
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_arrays[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T06:58:11.998686826Z",
     "start_time": "2024-01-09T06:58:11.951930759Z"
    }
   },
   "id": "7bea88cd69cceee7",
   "execution_count": 124
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "embedding_model.save_weights('new_mcode_similarity_model_weights.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T06:41:31.876171778Z",
     "start_time": "2024-01-09T06:41:30.391349808Z"
    }
   },
   "id": "fb6c4cb11591f965",
   "execution_count": 120
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 0s 346ms/step\n",
      "1/1 [==============================] - 0s 334ms/step\n",
      "1/1 [==============================] - 0s 331ms/step\n",
      "1/1 [==============================] - 0s 328ms/step\n",
      "1/1 [==============================] - 0s 335ms/step\n",
      "1/1 [==============================] - 0s 333ms/step\n",
      "1/1 [==============================] - 0s 338ms/step\n",
      "1/1 [==============================] - 0s 332ms/step\n",
      "1/1 [==============================] - 0s 337ms/step\n",
      "1/1 [==============================] - 0s 347ms/step\n",
      "1/1 [==============================] - 0s 344ms/step\n",
      "1/1 [==============================] - 0s 335ms/step\n",
      "1/1 [==============================] - 0s 333ms/step\n",
      "1/1 [==============================] - 0s 329ms/step\n",
      "1/1 [==============================] - 0s 333ms/step\n",
      "1/1 [==============================] - 0s 347ms/step\n",
      "1/1 [==============================] - 0s 353ms/step\n",
      "1/1 [==============================] - 0s 332ms/step\n",
      "1/1 [==============================] - 0s 334ms/step\n",
      "1/1 [==============================] - 0s 333ms/step\n",
      "1/1 [==============================] - 0s 332ms/step\n",
      "1/1 [==============================] - 0s 335ms/step\n",
      "1/1 [==============================] - 0s 335ms/step\n",
      "1/1 [==============================] - 0s 333ms/step\n",
      "1/1 [==============================] - 0s 332ms/step\n",
      "1/1 [==============================] - 0s 335ms/step\n",
      "1/1 [==============================] - 0s 348ms/step\n",
      "1/1 [==============================] - 0s 340ms/step\n",
      "1/1 [==============================] - 0s 334ms/step\n",
      "1/1 [==============================] - 0s 337ms/step\n",
      "1/1 [==============================] - 0s 342ms/step\n",
      "1/1 [==============================] - 0s 331ms/step\n",
      "1/1 [==============================] - 0s 339ms/step\n",
      "1/1 [==============================] - 0s 339ms/step\n",
      "1/1 [==============================] - 0s 340ms/step\n",
      "1/1 [==============================] - 0s 334ms/step\n",
      "1/1 [==============================] - 0s 337ms/step\n",
      "1/1 [==============================] - 0s 344ms/step\n",
      "1/1 [==============================] - 0s 343ms/step\n",
      "1/1 [==============================] - 0s 335ms/step\n",
      "1/1 [==============================] - 0s 338ms/step\n",
      "1/1 [==============================] - 0s 340ms/step\n",
      "1/1 [==============================] - 0s 338ms/step\n",
      "1/1 [==============================] - 0s 337ms/step\n",
      "1/1 [==============================] - 0s 340ms/step\n",
      "1/1 [==============================] - 0s 337ms/step\n",
      "1/1 [==============================] - 0s 335ms/step\n",
      "1/1 [==============================] - 0s 336ms/step\n",
      "1/1 [==============================] - 0s 340ms/step\n",
      "1/1 [==============================] - 0s 338ms/step\n",
      "1/1 [==============================] - 0s 338ms/step\n",
      "1/1 [==============================] - 0s 336ms/step\n",
      "1/1 [==============================] - 0s 340ms/step\n",
      "1/1 [==============================] - 0s 336ms/step\n",
      "1/1 [==============================] - 0s 339ms/step\n",
      "1/1 [==============================] - 0s 343ms/step\n",
      "1/1 [==============================] - 0s 339ms/step\n",
      "1/1 [==============================] - 0s 343ms/step\n",
      "1/1 [==============================] - 0s 336ms/step\n",
      "1/1 [==============================] - 0s 339ms/step\n",
      "1/1 [==============================] - 0s 343ms/step\n",
      "1/1 [==============================] - 0s 333ms/step\n",
      "1/1 [==============================] - 0s 340ms/step\n",
      "1/1 [==============================] - 0s 338ms/step\n",
      "1/1 [==============================] - 0s 337ms/step\n",
      "1/1 [==============================] - 0s 333ms/step\n",
      "1/1 [==============================] - 0s 337ms/step\n",
      "1/1 [==============================] - 0s 334ms/step\n",
      "1/1 [==============================] - 0s 335ms/step\n",
      "1/1 [==============================] - 0s 334ms/step\n",
      "1/1 [==============================] - 0s 338ms/step\n",
      "1/1 [==============================] - 0s 337ms/step\n",
      "1/1 [==============================] - 0s 330ms/step\n",
      "1/1 [==============================] - 0s 333ms/step\n",
      "1/1 [==============================] - 0s 334ms/step\n",
      "1/1 [==============================] - 0s 336ms/step\n",
      "1/1 [==============================] - 0s 332ms/step\n",
      "1/1 [==============================] - 0s 335ms/step\n",
      "1/1 [==============================] - 0s 334ms/step\n",
      "1/1 [==============================] - 0s 331ms/step\n",
      "1/1 [==============================] - 0s 330ms/step\n",
      "1/1 [==============================] - 0s 335ms/step\n",
      "1/1 [==============================] - 0s 335ms/step\n",
      "1/1 [==============================] - 0s 331ms/step\n",
      "1/1 [==============================] - 0s 331ms/step\n",
      "1/1 [==============================] - 0s 333ms/step\n",
      "1/1 [==============================] - 0s 332ms/step\n",
      "1/1 [==============================] - 0s 333ms/step\n",
      "1/1 [==============================] - 0s 334ms/step\n",
      "1/1 [==============================] - 0s 334ms/step\n",
      "1/1 [==============================] - 0s 331ms/step\n",
      "1/1 [==============================] - 0s 346ms/step\n",
      "1/1 [==============================] - 0s 334ms/step\n",
      "1/1 [==============================] - 0s 339ms/step\n",
      "1/1 [==============================] - 0s 329ms/step\n",
      "1/1 [==============================] - 0s 338ms/step\n",
      "1/1 [==============================] - 0s 351ms/step\n",
      "1/1 [==============================] - 0s 344ms/step\n",
      "1/1 [==============================] - 0s 346ms/step\n",
      "1/1 [==============================] - 0s 342ms/step\n",
      "1/1 [==============================] - 0s 368ms/step\n",
      "1/1 [==============================] - 0s 352ms/step\n",
      "1/1 [==============================] - 0s 333ms/step\n",
      "1/1 [==============================] - 0s 335ms/step\n",
      "1/1 [==============================] - 0s 334ms/step\n",
      "1/1 [==============================] - 0s 331ms/step\n",
      "1/1 [==============================] - 0s 330ms/step\n",
      "1/1 [==============================] - 0s 335ms/step\n",
      "1/1 [==============================] - 0s 347ms/step\n",
      "1/1 [==============================] - 0s 336ms/step\n",
      "1/1 [==============================] - 0s 335ms/step\n",
      "1/1 [==============================] - 0s 339ms/step\n",
      "1/1 [==============================] - 0s 335ms/step\n",
      "1/1 [==============================] - 0s 343ms/step\n",
      "1/1 [==============================] - 0s 332ms/step\n",
      "1/1 [==============================] - 0s 332ms/step\n",
      "1/1 [==============================] - 0s 329ms/step\n",
      "1/1 [==============================] - 0s 329ms/step\n",
      "1/1 [==============================] - 0s 344ms/step\n",
      "1/1 [==============================] - 0s 349ms/step\n",
      "1/1 [==============================] - 0s 348ms/step\n",
      "1/1 [==============================] - 0s 333ms/step\n",
      "1/1 [==============================] - 0s 341ms/step\n",
      "1/1 [==============================] - 0s 337ms/step\n",
      "1/1 [==============================] - 0s 339ms/step\n",
      "1/1 [==============================] - 0s 338ms/step\n",
      "1/1 [==============================] - 0s 337ms/step\n",
      "1/1 [==============================] - 0s 339ms/step\n",
      "1/1 [==============================] - 0s 362ms/step\n",
      "1/1 [==============================] - 0s 343ms/step\n",
      "1/1 [==============================] - 0s 343ms/step\n",
      "1/1 [==============================] - 0s 333ms/step\n",
      "1/1 [==============================] - 0s 337ms/step\n",
      "1/1 [==============================] - 0s 393ms/step\n",
      "1/1 [==============================] - 0s 411ms/step\n",
      "1/1 [==============================] - 0s 349ms/step\n",
      "1/1 [==============================] - 0s 332ms/step\n",
      "1/1 [==============================] - 0s 336ms/step\n",
      "1/1 [==============================] - 0s 335ms/step\n",
      "1/1 [==============================] - 0s 327ms/step\n",
      "1/1 [==============================] - 0s 331ms/step\n",
      "1/1 [==============================] - 0s 334ms/step\n",
      "1/1 [==============================] - 0s 332ms/step\n",
      "1/1 [==============================] - 0s 346ms/step\n",
      "1/1 [==============================] - 0s 340ms/step\n",
      "1/1 [==============================] - 0s 341ms/step\n",
      "1/1 [==============================] - 0s 335ms/step\n",
      "1/1 [==============================] - 0s 335ms/step\n",
      "1/1 [==============================] - 0s 335ms/step\n",
      "1/1 [==============================] - 0s 334ms/step\n",
      "1/1 [==============================] - 0s 327ms/step\n",
      "1/1 [==============================] - 0s 337ms/step\n",
      "1/1 [==============================] - 0s 345ms/step\n",
      "1/1 [==============================] - 0s 351ms/step\n",
      "1/1 [==============================] - 0s 352ms/step\n",
      "1/1 [==============================] - 0s 359ms/step\n",
      "1/1 [==============================] - 0s 362ms/step\n",
      "1/1 [==============================] - 0s 351ms/step\n",
      "1/1 [==============================] - 0s 368ms/step\n",
      "1/1 [==============================] - 1s 530ms/step\n",
      "1/1 [==============================] - 0s 458ms/step\n",
      "1/1 [==============================] - 0s 414ms/step\n",
      "1/1 [==============================] - 0s 434ms/step\n",
      "1/1 [==============================] - 0s 400ms/step\n",
      "1/1 [==============================] - 0s 397ms/step\n",
      "1/1 [==============================] - 0s 341ms/step\n",
      "1/1 [==============================] - 0s 431ms/step\n",
      "1/1 [==============================] - 0s 366ms/step\n",
      "1/1 [==============================] - 0s 420ms/step\n",
      "1/1 [==============================] - 1s 532ms/step\n",
      "1/1 [==============================] - 0s 349ms/step\n",
      "1/1 [==============================] - 0s 497ms/step\n",
      "1/1 [==============================] - 0s 451ms/step\n",
      "1/1 [==============================] - 0s 471ms/step\n",
      "1/1 [==============================] - 1s 500ms/step\n",
      "1/1 [==============================] - 0s 484ms/step\n",
      "1/1 [==============================] - 0s 334ms/step\n",
      "1/1 [==============================] - 0s 429ms/step\n",
      "1/1 [==============================] - 0s 427ms/step\n",
      "1/1 [==============================] - 0s 396ms/step\n",
      "1/1 [==============================] - 0s 421ms/step\n",
      "1/1 [==============================] - 0s 481ms/step\n",
      "1/1 [==============================] - 0s 466ms/step\n",
      "1/1 [==============================] - 0s 389ms/step\n",
      "1/1 [==============================] - 0s 334ms/step\n",
      "1/1 [==============================] - 0s 386ms/step\n",
      "1/1 [==============================] - 0s 384ms/step\n",
      "1/1 [==============================] - 0s 390ms/step\n",
      "1/1 [==============================] - 0s 364ms/step\n",
      "1/1 [==============================] - 0s 404ms/step\n",
      "1/1 [==============================] - 0s 432ms/step\n",
      "1/1 [==============================] - 0s 450ms/step\n",
      "1/1 [==============================] - 0s 338ms/step\n",
      "1/1 [==============================] - 0s 335ms/step\n",
      "1/1 [==============================] - 0s 336ms/step\n",
      "1/1 [==============================] - 0s 336ms/step\n",
      "1/1 [==============================] - 0s 337ms/step\n",
      "1/1 [==============================] - 0s 335ms/step\n",
      "1/1 [==============================] - 0s 336ms/step\n",
      "1/1 [==============================] - 0s 340ms/step\n",
      "1/1 [==============================] - 0s 347ms/step\n",
      "1/1 [==============================] - 0s 352ms/step\n",
      "1/1 [==============================] - 0s 348ms/step\n",
      "1/1 [==============================] - 0s 349ms/step\n",
      "1/1 [==============================] - 0s 337ms/step\n",
      "1/1 [==============================] - 0s 348ms/step\n",
      "1/1 [==============================] - 0s 350ms/step\n",
      "1/1 [==============================] - 0s 356ms/step\n",
      "1/1 [==============================] - 0s 331ms/step\n",
      "1/1 [==============================] - 0s 340ms/step\n",
      "1/1 [==============================] - 0s 406ms/step\n",
      "1/1 [==============================] - 0s 484ms/step\n",
      "1/1 [==============================] - 1s 506ms/step\n",
      "1/1 [==============================] - 0s 464ms/step\n",
      "1/1 [==============================] - 0s 419ms/step\n",
      "1/1 [==============================] - 0s 425ms/step\n",
      "1/1 [==============================] - 0s 406ms/step\n",
      "1/1 [==============================] - 0s 401ms/step\n",
      "1/1 [==============================] - 0s 417ms/step\n",
      "1/1 [==============================] - 0s 467ms/step\n",
      "1/1 [==============================] - 0s 437ms/step\n",
      "1/1 [==============================] - 0s 396ms/step\n",
      "1/1 [==============================] - 0s 400ms/step\n",
      "1/1 [==============================] - 0s 408ms/step\n",
      "1/1 [==============================] - 0s 392ms/step\n",
      "1/1 [==============================] - 0s 391ms/step\n",
      "1/1 [==============================] - 0s 390ms/step\n",
      "1/1 [==============================] - 0s 415ms/step\n",
      "1/1 [==============================] - 0s 453ms/step\n",
      "1/1 [==============================] - 0s 443ms/step\n",
      "1/1 [==============================] - 0s 405ms/step\n",
      "1/1 [==============================] - 0s 398ms/step\n",
      "1/1 [==============================] - 0s 386ms/step\n",
      "1/1 [==============================] - 0s 391ms/step\n",
      "1/1 [==============================] - 0s 433ms/step\n",
      "1/1 [==============================] - 0s 449ms/step\n",
      "1/1 [==============================] - 0s 460ms/step\n",
      "1/1 [==============================] - 0s 394ms/step\n",
      "1/1 [==============================] - 0s 394ms/step\n",
      "1/1 [==============================] - 0s 384ms/step\n",
      "1/1 [==============================] - 0s 413ms/step\n",
      "1/1 [==============================] - 0s 351ms/step\n",
      "1/1 [==============================] - 0s 440ms/step\n",
      "1/1 [==============================] - 0s 483ms/step\n",
      "1/1 [==============================] - 0s 435ms/step\n",
      "1/1 [==============================] - 0s 396ms/step\n",
      "1/1 [==============================] - 0s 381ms/step\n",
      "1/1 [==============================] - 0s 371ms/step\n",
      "1/1 [==============================] - 0s 394ms/step\n",
      "1/1 [==============================] - 0s 405ms/step\n",
      "1/1 [==============================] - 0s 473ms/step\n",
      "1/1 [==============================] - 0s 454ms/step\n",
      "1/1 [==============================] - 0s 397ms/step\n",
      "1/1 [==============================] - 0s 395ms/step\n",
      "1/1 [==============================] - 0s 400ms/step\n",
      "1/1 [==============================] - 0s 398ms/step\n",
      "1/1 [==============================] - 0s 418ms/step\n",
      "1/1 [==============================] - 0s 361ms/step\n",
      "1/1 [==============================] - 0s 433ms/step\n",
      "1/1 [==============================] - 0s 459ms/step\n",
      "1/1 [==============================] - 0s 445ms/step\n",
      "1/1 [==============================] - 0s 405ms/step\n",
      "1/1 [==============================] - 0s 376ms/step\n",
      "1/1 [==============================] - 0s 387ms/step\n",
      "1/1 [==============================] - 0s 385ms/step\n",
      "1/1 [==============================] - 0s 406ms/step\n",
      "1/1 [==============================] - 0s 445ms/step\n",
      "1/1 [==============================] - 0s 467ms/step\n",
      "1/1 [==============================] - 0s 434ms/step\n",
      "1/1 [==============================] - 0s 394ms/step\n",
      "1/1 [==============================] - 0s 335ms/step\n",
      "1/1 [==============================] - 0s 397ms/step\n",
      "1/1 [==============================] - 0s 402ms/step\n",
      "1/1 [==============================] - 0s 399ms/step\n",
      "1/1 [==============================] - 0s 420ms/step\n",
      "1/1 [==============================] - 0s 346ms/step\n",
      "1/1 [==============================] - 0s 423ms/step\n",
      "1/1 [==============================] - 0s 364ms/step\n",
      "1/1 [==============================] - 0s 454ms/step\n",
      "1/1 [==============================] - 0s 391ms/step\n",
      "1/1 [==============================] - 0s 475ms/step\n",
      "1/1 [==============================] - 1s 509ms/step\n",
      "1/1 [==============================] - 0s 389ms/step\n",
      "1/1 [==============================] - 0s 431ms/step\n",
      "1/1 [==============================] - 0s 368ms/step\n",
      "1/1 [==============================] - 0s 385ms/step\n",
      "1/1 [==============================] - 0s 333ms/step\n",
      "1/1 [==============================] - 0s 468ms/step\n",
      "1/1 [==============================] - 0s 499ms/step\n",
      "1/1 [==============================] - 0s 454ms/step\n",
      "1/1 [==============================] - 0s 405ms/step\n",
      "1/1 [==============================] - 0s 416ms/step\n",
      "1/1 [==============================] - 0s 394ms/step\n",
      "1/1 [==============================] - 0s 393ms/step\n",
      "1/1 [==============================] - 0s 391ms/step\n",
      "1/1 [==============================] - 0s 357ms/step\n",
      "1/1 [==============================] - 0s 425ms/step\n",
      "1/1 [==============================] - 0s 481ms/step\n",
      "1/1 [==============================] - 0s 452ms/step\n",
      "1/1 [==============================] - 0s 339ms/step\n",
      "1/1 [==============================] - 0s 396ms/step\n",
      "1/1 [==============================] - 0s 410ms/step\n",
      "1/1 [==============================] - 0s 401ms/step\n",
      "1/1 [==============================] - 0s 401ms/step\n",
      "1/1 [==============================] - 0s 402ms/step\n",
      "1/1 [==============================] - 0s 396ms/step\n",
      "1/1 [==============================] - 0s 379ms/step\n",
      "1/1 [==============================] - 0s 486ms/step\n",
      "1/1 [==============================] - 0s 419ms/step\n",
      "1/1 [==============================] - 0s 409ms/step\n",
      "1/1 [==============================] - 0s 419ms/step\n",
      "1/1 [==============================] - 0s 405ms/step\n",
      "1/1 [==============================] - 0s 442ms/step\n",
      "1/1 [==============================] - 0s 437ms/step\n",
      "1/1 [==============================] - 0s 409ms/step\n",
      "1/1 [==============================] - 0s 360ms/step\n",
      "1/1 [==============================] - 0s 431ms/step\n",
      "1/1 [==============================] - 0s 469ms/step\n",
      "1/1 [==============================] - 0s 464ms/step\n",
      "1/1 [==============================] - 0s 407ms/step\n",
      "1/1 [==============================] - 0s 417ms/step\n",
      "1/1 [==============================] - 0s 405ms/step\n",
      "1/1 [==============================] - 0s 414ms/step\n",
      "1/1 [==============================] - 0s 447ms/step\n",
      "1/1 [==============================] - 0s 456ms/step\n",
      "1/1 [==============================] - 0s 473ms/step\n",
      "1/1 [==============================] - 0s 413ms/step\n",
      "1/1 [==============================] - 0s 419ms/step\n",
      "1/1 [==============================] - 0s 402ms/step\n",
      "1/1 [==============================] - 0s 415ms/step\n",
      "1/1 [==============================] - 0s 415ms/step\n",
      "1/1 [==============================] - 0s 436ms/step\n",
      "1/1 [==============================] - 0s 431ms/step\n",
      "1/1 [==============================] - 0s 375ms/step\n",
      "1/1 [==============================] - 0s 413ms/step\n",
      "1/1 [==============================] - 0s 393ms/step\n",
      "1/1 [==============================] - 0s 332ms/step\n",
      "1/1 [==============================] - 0s 399ms/step\n",
      "1/1 [==============================] - 0s 398ms/step\n",
      "1/1 [==============================] - 0s 405ms/step\n",
      "1/1 [==============================] - 0s 396ms/step\n",
      "1/1 [==============================] - 0s 392ms/step\n",
      "1/1 [==============================] - 0s 374ms/step\n",
      "1/1 [==============================] - 0s 447ms/step\n",
      "1/1 [==============================] - 0s 483ms/step\n",
      "1/1 [==============================] - 0s 403ms/step\n",
      "1/1 [==============================] - 0s 407ms/step\n",
      "1/1 [==============================] - 0s 401ms/step\n",
      "1/1 [==============================] - 0s 389ms/step\n",
      "1/1 [==============================] - 0s 392ms/step\n",
      "1/1 [==============================] - 0s 425ms/step\n",
      "1/1 [==============================] - 0s 360ms/step\n",
      "1/1 [==============================] - 0s 490ms/step\n",
      "1/1 [==============================] - 0s 461ms/step\n",
      "1/1 [==============================] - 0s 403ms/step\n",
      "1/1 [==============================] - 0s 393ms/step\n",
      "1/1 [==============================] - 0s 381ms/step\n",
      "1/1 [==============================] - 0s 392ms/step\n",
      "1/1 [==============================] - 0s 480ms/step\n",
      "1/1 [==============================] - 0s 471ms/step\n",
      "1/1 [==============================] - 0s 456ms/step\n",
      "1/1 [==============================] - 0s 340ms/step\n",
      "1/1 [==============================] - 0s 339ms/step\n",
      "1/1 [==============================] - 0s 337ms/step\n",
      "1/1 [==============================] - 0s 339ms/step\n",
      "1/1 [==============================] - 0s 341ms/step\n",
      "1/1 [==============================] - 0s 338ms/step\n",
      "1/1 [==============================] - 0s 344ms/step\n",
      "1/1 [==============================] - 0s 343ms/step\n",
      "1/1 [==============================] - 0s 346ms/step\n",
      "1/1 [==============================] - 0s 340ms/step\n",
      "1/1 [==============================] - 0s 339ms/step\n",
      "1/1 [==============================] - 0s 341ms/step\n",
      "1/1 [==============================] - 0s 338ms/step\n",
      "1/1 [==============================] - 0s 340ms/step\n",
      "1/1 [==============================] - 0s 338ms/step\n",
      "1/1 [==============================] - 0s 345ms/step\n",
      "1/1 [==============================] - 0s 338ms/step\n",
      "1/1 [==============================] - 0s 351ms/step\n",
      "1/1 [==============================] - 0s 343ms/step\n",
      "1/1 [==============================] - 0s 345ms/step\n",
      "1/1 [==============================] - 0s 341ms/step\n",
      "1/1 [==============================] - 0s 336ms/step\n",
      "1/1 [==============================] - 0s 339ms/step\n",
      "1/1 [==============================] - 0s 343ms/step\n",
      "1/1 [==============================] - 0s 344ms/step\n",
      "1/1 [==============================] - 0s 349ms/step\n",
      "1/1 [==============================] - 0s 345ms/step\n",
      "1/1 [==============================] - 0s 338ms/step\n",
      "1/1 [==============================] - 0s 339ms/step\n",
      "1/1 [==============================] - 0s 339ms/step\n",
      "1/1 [==============================] - 0s 341ms/step\n",
      "1/1 [==============================] - 0s 339ms/step\n",
      "1/1 [==============================] - 0s 338ms/step\n",
      "1/1 [==============================] - 0s 340ms/step\n",
      "1/1 [==============================] - 0s 349ms/step\n",
      "1/1 [==============================] - 0s 343ms/step\n",
      "1/1 [==============================] - 0s 341ms/step\n",
      "1/1 [==============================] - 0s 336ms/step\n",
      "1/1 [==============================] - 0s 350ms/step\n",
      "1/1 [==============================] - 0s 343ms/step\n",
      "1/1 [==============================] - 0s 341ms/step\n",
      "1/1 [==============================] - 0s 339ms/step\n",
      "1/1 [==============================] - 0s 342ms/step\n",
      "1/1 [==============================] - 0s 352ms/step\n",
      "1/1 [==============================] - 0s 361ms/step\n",
      "1/1 [==============================] - 0s 368ms/step\n",
      "1/1 [==============================] - 0s 386ms/step\n",
      "1/1 [==============================] - 0s 382ms/step\n",
      "1/1 [==============================] - 0s 381ms/step\n",
      "1/1 [==============================] - 0s 383ms/step\n",
      "1/1 [==============================] - 0s 376ms/step\n",
      "1/1 [==============================] - 0s 380ms/step\n",
      "1/1 [==============================] - 0s 382ms/step\n",
      "1/1 [==============================] - 0s 378ms/step\n",
      "1/1 [==============================] - 0s 383ms/step\n",
      "1/1 [==============================] - 0s 380ms/step\n",
      "1/1 [==============================] - 0s 386ms/step\n",
      "1/1 [==============================] - 0s 385ms/step\n",
      "1/1 [==============================] - 0s 381ms/step\n",
      "1/1 [==============================] - 0s 382ms/step\n",
      "1/1 [==============================] - 0s 386ms/step\n",
      "1/1 [==============================] - 0s 375ms/step\n",
      "1/1 [==============================] - 0s 395ms/step\n",
      "1/1 [==============================] - 0s 395ms/step\n",
      "1/1 [==============================] - 0s 387ms/step\n",
      "1/1 [==============================] - 0s 383ms/step\n",
      "1/1 [==============================] - 0s 387ms/step\n",
      "1/1 [==============================] - 0s 376ms/step\n",
      "1/1 [==============================] - 0s 385ms/step\n",
      "1/1 [==============================] - 0s 410ms/step\n",
      "1/1 [==============================] - 0s 404ms/step\n",
      "1/1 [==============================] - 0s 389ms/step\n",
      "1/1 [==============================] - 0s 376ms/step\n",
      "1/1 [==============================] - 0s 380ms/step\n",
      "1/1 [==============================] - 0s 381ms/step\n",
      "1/1 [==============================] - 0s 387ms/step\n",
      "1/1 [==============================] - 0s 379ms/step\n",
      "1/1 [==============================] - 0s 374ms/step\n",
      "1/1 [==============================] - 0s 383ms/step\n",
      "1/1 [==============================] - 0s 382ms/step\n",
      "1/1 [==============================] - 0s 381ms/step\n",
      "1/1 [==============================] - 0s 392ms/step\n",
      "1/1 [==============================] - 0s 381ms/step\n",
      "1/1 [==============================] - 0s 380ms/step\n",
      "1/1 [==============================] - 0s 388ms/step\n",
      "1/1 [==============================] - 0s 389ms/step\n",
      "1/1 [==============================] - 0s 389ms/step\n",
      "1/1 [==============================] - 0s 388ms/step\n",
      "1/1 [==============================] - 0s 386ms/step\n",
      "1/1 [==============================] - 0s 385ms/step\n",
      "1/1 [==============================] - 0s 375ms/step\n",
      "1/1 [==============================] - 0s 385ms/step\n",
      "1/1 [==============================] - 0s 398ms/step\n",
      "1/1 [==============================] - 0s 386ms/step\n",
      "1/1 [==============================] - 0s 385ms/step\n",
      "1/1 [==============================] - 0s 384ms/step\n",
      "1/1 [==============================] - 0s 386ms/step\n",
      "1/1 [==============================] - 0s 380ms/step\n",
      "1/1 [==============================] - 0s 388ms/step\n",
      "1/1 [==============================] - 0s 381ms/step\n",
      "1/1 [==============================] - 0s 384ms/step\n",
      "1/1 [==============================] - 0s 393ms/step\n",
      "1/1 [==============================] - 0s 382ms/step\n",
      "1/1 [==============================] - 0s 386ms/step\n",
      "1/1 [==============================] - 0s 386ms/step\n"
     ]
    }
   ],
   "source": [
    "# Rebuild the embedding model architecture (matching the saved model)\n",
    "def codebert_embedding_model():\n",
    "    input_ids = tf.keras.layers.Input(shape=(512,), dtype='int32')\n",
    "    attention_mask = tf.keras.layers.Input(shape=(512,), dtype='int32')\n",
    "    embeddings = codebert_model(input_ids, attention_mask=attention_mask)[0][:,0,:]\n",
    "    return tf.keras.Model(inputs=[input_ids, attention_mask], outputs=embeddings)\n",
    "\n",
    "# Instantiate the model\n",
    "embedding_model = codebert_embedding_model()\n",
    "\n",
    "# Load the weights from the saved model\n",
    "embedding_model.load_weights(\"new_mcode_similarity_model_weights.h5\")\n",
    "\n",
    "def get_embeddings(file_paths, tokenizer, embedding_model):\n",
    "    embeddings = []\n",
    "    for path in file_paths:\n",
    "        input_ids, attention_mask = tokenize_code(path, tokenizer)  \n",
    "        embedding = embedding_model.predict([input_ids, attention_mask])\n",
    "        embeddings.append(embedding)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "file_embeddings = get_embeddings(all_arrays, tokenizer, embedding_model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T06:45:28.820775820Z",
     "start_time": "2024-01-09T06:42:04.064219441Z"
    }
   },
   "id": "33f7a7bab9f3aa51",
   "execution_count": 121
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([[[-0.14163618,  0.09809396,  0.00482307, ..., -0.05911174,\n         -0.5362416 ,  0.5404009 ]],\n\n       [[-0.12784305,  0.07232494,  0.00668415, ..., -0.12009836,\n         -0.5672197 ,  0.51583123]],\n\n       [[-0.15808322,  0.09880828,  0.03295183, ..., -0.09101688,\n         -0.5153878 ,  0.52900887]],\n\n       ...,\n\n       [[-0.11016048,  0.02217015, -0.00234284, ..., -0.07208943,\n         -0.5958338 ,  0.5298103 ]],\n\n       [[-0.16940765, -0.02671118, -0.01802878, ..., -0.07803576,\n         -0.57609975,  0.4933287 ]],\n\n       [[-0.15300442, -0.00710349, -0.00725231, ..., -0.03707576,\n         -0.5625479 ,  0.5090958 ]]], dtype=float32)"
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T06:47:51.338918455Z",
     "start_time": "2024-01-09T06:47:51.296856500Z"
    }
   },
   "id": "5129bc1642781ed5",
   "execution_count": 122
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "# Rebuild the model\n",
    "def codebert_embedding_model():\n",
    "    input_ids = tf.keras.layers.Input(shape=(512,), dtype='int32')\n",
    "    attention_mask = tf.keras.layers.Input(shape=(512,), dtype='int32')\n",
    "    embeddings = codebert_model(input_ids, attention_mask=attention_mask)[0][:,0,:]\n",
    "    return tf.keras.Model(inputs=[input_ids, attention_mask], outputs=embeddings)\n",
    "\n",
    "embedding_model = codebert_embedding_model()\n",
    "embedding_model.load_weights(\"new_mcode_similarity_model_weights.h5\")\n",
    "\n",
    "# Tokenize and Embedding Generation Function\n",
    "def tokenize_code(file_path, tokenizer):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        code = file.read()\n",
    "    inputs = tokenizer(code, return_tensors=\"tf\", truncation=True, padding='max_length', max_length=512)\n",
    "    return tf.squeeze(inputs['input_ids']), tf.squeeze(inputs['attention_mask'])\n",
    "\n",
    "def generate_embedding(file_path, tokenizer, model):\n",
    "    input_ids, attention_mask = tokenize_code(file_path, tokenizer)\n",
    "    embedding = model([tf.expand_dims(input_ids, 0), tf.expand_dims(attention_mask, 0)])\n",
    "    return embedding[0]\n",
    "\n",
    "# Generate embeddings for all files\n",
    "file_embeddings = {file: generate_embedding(file, tokenizer, embedding_model) for file in all_arrays}\n",
    "\n",
    "# Custom Similarity Function\n",
    "def custom_similarity(embedding1, embedding2):\n",
    "    distance = tf.norm(embedding1 - embedding2)\n",
    "    similarity = tf.exp(-distance)\n",
    "    return similarity.numpy()\n",
    "\n",
    "# Prepare pairs for comparison and calculate similarity\n",
    "comparisons = []\n",
    "for i in range(len(all_arrays)):\n",
    "    for j in range(i+1, len(all_arrays)):\n",
    "        file1, file2 = all_arrays[i], all_arrays[j]\n",
    "        similarity_score = custom_similarity(file_embeddings[file1], file_embeddings[file2])\n",
    "        comparisons.append([file1, file2, similarity_score])\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(comparisons, columns=['file1', 'file2', 'similarity'])\n",
    "\n",
    "# Display or save the DataFrame\n",
    "print(df.head())\n",
    "# df.to_csv('file_similarity_scores.csv', index=False)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8639cc82328ee5d",
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "# Assuming the functions codebert_embedding_model, tokenize_code, and generate_embedding are defined as before\n",
    "\n",
    "embedding_model = codebert_embedding_model()\n",
    "embedding_model.load_weights(\"new_mcode_similarity_model_weights.h5\")\n",
    "\n",
    "\n",
    "# Generate embeddings for all files\n",
    "file_embeddings = {file: generate_embedding(file, tokenizer, embedding_model) for file in all_arrays}\n",
    "\n",
    "# Custom Similarity Function\n",
    "def custom_similarity(embedding1, embedding2):\n",
    "    distance = tf.norm(embedding1 - embedding2)\n",
    "    similarity = tf.exp(-distance)\n",
    "    return similarity.numpy()\n",
    "\n",
    "# Define a threshold for similarity\n",
    "similarity_threshold = 0.5  # You may need to adjust this value\n",
    "\n",
    "# Prepare pairs for comparison, calculate similarity, and label\n",
    "comparisons = []\n",
    "for i in range(len(all_arrays)):\n",
    "    for j in range(i+1, len(all_arrays)):\n",
    "        file1, file2 = all_arrays[i], all_arrays[j]\n",
    "        similarity_score = custom_similarity(file_embeddings[file1], file_embeddings[file2])\n",
    "        label = \"yes\" if similarity_score >= similarity_threshold else \"no\"\n",
    "        comparisons.append([file1, file2, label])\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(comparisons, columns=['file1', 'file2', 'label'])\n",
    "\n",
    "# Display or save the DataFrame\n",
    "print(df.head())\n",
    "# df.to_csv('file_similarity_labels.csv', index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-08T21:03:40.607382773Z",
     "start_time": "2024-01-08T21:03:40.606487209Z"
    }
   },
   "id": "cf70ebb423b409bb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.to_csv('file_similarity_dataset.csv', index=False)  # To save to a CSV file"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-08T21:03:40.606575572Z"
    }
   },
   "id": "2fbe43a413fd61db",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import GRU, Dense, Input, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "embedding_model = codebert_embedding_model()\n",
    "embedding_model.load_weights(\"new_mcode_similarity_model_weights.h5\")\n",
    "# Generate embeddings for each file\n",
    "#file_embeddings = get_embeddings(all_arrays, tokenizer, embedding_model)\n",
    "\n",
    "# Prepare pairs of embeddings and labels for GRU\n",
    "def prepare_pairs_and_labels(df, embeddings):\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    for _, row in df.iterrows():\n",
    "        file1_index = np.where(all_arrays == row['file1'])[0][0]\n",
    "        file2_index = np.where(all_arrays == row['file2'])[0][0]\n",
    "        pairs.append(np.concatenate([embeddings[file1_index], embeddings[file2_index]]))\n",
    "        labels.append(1 if row['label'] == 'yes' else 0)\n",
    "    return np.array(pairs), np.array(labels)\n",
    "\n",
    "pairs, labels = prepare_pairs_and_labels(df, file_embeddings)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_pairs, test_pairs, train_labels, test_labels = train_test_split(pairs, labels, test_size=0.2, random_state=42)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T07:03:17.516606308Z",
     "start_time": "2024-01-09T07:02:59.374332305Z"
    }
   },
   "id": "fe803adad51ef205",
   "execution_count": 125
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([[[-1.21589109e-01,  1.28530106e-02, -3.38866711e-02, ...,\n         -3.04514468e-02, -6.07127786e-01,  5.62189639e-01],\n        [-1.51905447e-01,  5.58534749e-02,  4.51145880e-02, ...,\n         -7.75929466e-02, -5.33326745e-01,  4.63465035e-01]],\n\n       [[-1.60097748e-01,  3.65802273e-03, -5.70366196e-02, ...,\n         -8.92864540e-02, -5.53961277e-01,  4.93913382e-01],\n        [-1.47676274e-01, -3.36678363e-02, -4.36971411e-02, ...,\n         -1.07287005e-01, -6.00591600e-01,  4.52217579e-01]],\n\n       [[-8.50146860e-02,  2.63041295e-02, -4.91912887e-02, ...,\n         -1.78905278e-02, -5.55695891e-01,  4.77093816e-01],\n        [-1.23309217e-01,  7.81061426e-02,  3.29925679e-02, ...,\n         -4.72574532e-02, -5.66953540e-01,  5.34310162e-01]],\n\n       ...,\n\n       [[-1.14190742e-01,  3.52792740e-02,  4.34295721e-02, ...,\n         -3.93711627e-02, -5.73682904e-01,  5.40191948e-01],\n        [-9.79688913e-02,  2.48303618e-02,  3.11865658e-02, ...,\n          2.22712606e-02, -6.09623313e-01,  5.55661857e-01]],\n\n       [[-1.27843052e-01,  7.23249391e-02,  6.68414682e-03, ...,\n         -1.20098360e-01, -5.67219675e-01,  5.15831232e-01],\n        [-1.63449481e-01, -1.81116946e-02,  2.21381336e-03, ...,\n         -3.06541622e-02, -6.02712095e-01,  5.22079647e-01]],\n\n       [[-1.17091455e-01,  6.05561025e-02,  1.59777440e-02, ...,\n         -5.18836826e-02, -5.78744173e-01,  5.57174206e-01],\n        [-9.39977318e-02,  3.34511921e-02,  6.39278442e-05, ...,\n         -4.51971889e-02, -5.84404111e-01,  5.23009598e-01]]],\n      dtype=float32)"
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pairs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T07:03:28.533794572Z",
     "start_time": "2024-01-09T07:03:28.509844524Z"
    }
   },
   "id": "590652fa56fb6405",
   "execution_count": 126
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2177/2177 [==============================] - 14s 6ms/step - loss: 0.0271 - accuracy: 0.9954 - val_loss: 0.0251 - val_accuracy: 0.9961\n",
      "Epoch 2/10\n",
      "2177/2177 [==============================] - 12s 5ms/step - loss: 0.0251 - accuracy: 0.9959 - val_loss: 0.0241 - val_accuracy: 0.9961\n",
      "Epoch 3/10\n",
      "2177/2177 [==============================] - 12s 5ms/step - loss: 0.0241 - accuracy: 0.9959 - val_loss: 0.0234 - val_accuracy: 0.9961\n",
      "Epoch 4/10\n",
      "2177/2177 [==============================] - 12s 5ms/step - loss: 0.0240 - accuracy: 0.9959 - val_loss: 0.0233 - val_accuracy: 0.9961\n",
      "Epoch 5/10\n",
      "2177/2177 [==============================] - 12s 5ms/step - loss: 0.0237 - accuracy: 0.9959 - val_loss: 0.0243 - val_accuracy: 0.9961\n",
      "Epoch 6/10\n",
      "2177/2177 [==============================] - 12s 5ms/step - loss: 0.0236 - accuracy: 0.9959 - val_loss: 0.0237 - val_accuracy: 0.9961\n",
      "Epoch 7/10\n",
      "2177/2177 [==============================] - 12s 5ms/step - loss: 0.0233 - accuracy: 0.9959 - val_loss: 0.0229 - val_accuracy: 0.9961\n",
      "Epoch 8/10\n",
      "2177/2177 [==============================] - 12s 5ms/step - loss: 0.0233 - accuracy: 0.9959 - val_loss: 0.0231 - val_accuracy: 0.9961\n",
      "Epoch 9/10\n",
      "2177/2177 [==============================] - 12s 5ms/step - loss: 0.0230 - accuracy: 0.9959 - val_loss: 0.0227 - val_accuracy: 0.9961\n",
      "Epoch 10/10\n",
      "2177/2177 [==============================] - 12s 5ms/step - loss: 0.0228 - accuracy: 0.9959 - val_loss: 0.0225 - val_accuracy: 0.9961\n",
      "681/681 [==============================] - 2s 2ms/step - loss: 0.0229 - accuracy: 0.9960\n",
      "Test Loss: 0.022859524935483932, Test Accuracy: 0.9960023760795593\n"
     ]
    }
   ],
   "source": [
    "# Define GRU model\n",
    "def build_gru_model(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    gru_layer = GRU(64, return_sequences=False)(input_layer)\n",
    "    output_layer = Dense(1, activation='sigmoid')(gru_layer)\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "gru_model = build_gru_model(train_pairs.shape[1:])  # Adjust input shape as necessary\n",
    "\n",
    "# Train the model\n",
    "gru_model.fit(train_pairs, train_labels, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluation = gru_model.evaluate(test_pairs, test_labels)\n",
    "print(f\"Test Loss: {evaluation[0]}, Test Accuracy: {evaluation[1]}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T07:06:54.521346411Z",
     "start_time": "2024-01-09T07:04:51.995654126Z"
    }
   },
   "id": "88aa73952cde54e6",
   "execution_count": 129
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-08T21:03:40.573840167Z",
     "start_time": "2024-01-08T21:03:40.558050849Z"
    }
   },
   "id": "50fa4f5aa0f1baf2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
